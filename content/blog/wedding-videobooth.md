Title: Creating a magical videobooth for a wedding
Date: 2018-10-14
Slug: wedding-videobooth
Summary: Some technical and aesthetic notes about building a surprise for a the wedding of some friends
Status: draft

*This post was written jointly with
[Rachel Hwang](http://rahwang.strikingly.com/), who worked on this
project with me.*

Our friends [Emily](https://www.instagram.com/emilytishay) and
[John](http://www.daz.zone/) got married recently. A few months
beforehand, we decided to make them a surprise in the form of an
interactive computer art installation to be deployed at their
wedding. The guests used it to create some amazing things:

<video width="800" height="500" autoplay loop src="/assets/videobooth/josh_emily_heart_short.mp4"></video>

<video width="800" height="500" autoplay loop src="/assets/videobooth/riva_dance_short.mp4"></video>

<video width="800" height="500" autoplay loop src="/assets/videobooth/nick_star_short.mp4"></video>

The rest of this post is about what we did to make it, some of the
technical and aesthetic problems we ran into along the way, and how we
solved them.


## Getting started

We started off by thinking about what the important characteristics of
the final product would be, and brainstorming some ideas. Our goal was
to make something that, in addition to being a fun interactive toy for
guests at the wedding, would also produce artifacts that could be
compiled into a "guestbook" of sorts for Emily and John.

We settled on the idea of guests moving colored lights through the air
like wands to draw interesting visual effects on a screen in front of
them, by means of a computer vision algorithm tracking the lights and
rendering effects based on their positions. You'd walk into the booth,
type a message to Emily and John to be included with your video, and
then have 30 seconds to record a video of yourself making something
beautiful, funny, or interesting using the light wands.

## Settling on a development environment

Now that we had a rough idea, we had to actually start coding up some
prototypes. Unfortunately, both of us struggle with getting over the
initial "activation energy" hump in starting projects like this, but
find it much easier to keep going once we've started.

To help with this, we ended up choosing [Glitch](https://glitch.com/)
as our development environment, which was an amazing choice. Glitch
makes the initial phases of working on a web project with someone else
*incredibly easy*. Collaboration? Just type in the shared editor and
everyone else's view updates in real-time. Deployment? Already done,
just go to `my-project-name.glitch.me` and see the code you just typed
running live. We didn't believe the hype about Glitch before this
project, but we're totally sold now[^1].

We also chose Glitch because it's web-based. Because the web is such a
flexible, powerful development platform, and because we're both
familiar with it, we were more confident in our ability to do some of
the ancillary but important parts of the project (user interface for
typing messages, capturing and saving videos) than if we had chosen
something like [Processing](https://processing.org/).


## Color tracking

As a prerequisite for doing any of this, we needed a way to track the
positions of colored objects in a video stream. We poked around for a
while and found [tracking.js](https://trackingjs.com/), a library of
JavaScript computer vision algorithms. After a bit of trial and error
we managed to use it's color tracking facilities to get a very basic
demo working:

<video autoplay loop
src="/assets/videobooth/video.mp4"></video>

This was pretty much just the tracking.js
[color tracking example](https://trackingjs.com/examples/color_camera.html)
with the parameters tweaked so it tracks orange rather
cyan/magenta/yellow, but it was a start.

## Making it beautiful

Now we needed to draw something more appealing than a bounding box in
response to the user's movements. We decided to use
[p5.js](https://p5js.org/) since we were familiar with it. We added p5
to our codebase and tweaked the color tracking code to use `canvas`
and `video` elements generated by p5, and to be driven by p5's render
loop. We started off by drawing a basic flower pattern. We don't have
video of this stage unfortunately, but here are some photos:

![James with flower](/assets/videobooth/james_with_yellow_flower.png)
![Rachel with flower](/assets/videobooth/rachel_with_yellow_flower.png)

Again, not the most beautiful thing, but better than a bounding box!
It's also a proof of concept that we could integrate p5 with
tracking.js and start working towards making something better.

In searching for inspiration, Golan Levin's
[Yellowtail](http://flong.com/projects/yellowtail/), which is one of
the examples included with Processing, caught our eye. We though the
strokes it generates might look nice if we could adapt them to be
drawn by the tracked blobs on the camera rather than the mouse.

We were able to find
[a Yellowtail implementation using p5](https://n1ckfg.github.io/yellowtails/p5js/)
and the
[source code for it](https://github.com/n1ckfg/yellowtails). We
decided to try and integrate this code into our project, adapting the
callbacks for things like clicks mouse movement to instead by
triggered by the blobs detected by the color tracker. This proved to
be pretty challenging and have a lot of pitfalls and visually
interesting bugs, for example:

![James demoing bug](/assets/videobooth/james_flower_glitch.png)

Eventually, we managed to get it working:

![James Yellowtail image](/assets/videobooth/james_first_yellowtail.png)

This was very exciting! It was finally starting to feel a bit like
what we had in out mind's eye when we started this project.

## Hardware choices

One of the big hurdles remaining was figuring out what hardware we
were going to use as the colored objects to be tracked. Up until this
point, we'd been testing with ceiling lights and various shiny objects
in out apartments, but that wasn't going to cut it for the final
version. We wanted whatever we settled on to have a few
characteristics:

1. No false positives, i.e. someone's clothes or body won't be
   mistaken for it, resulting in spurious drawings.
2. Works well in a variety of light conditions. This was important,
   since we weren't exactly sure what the ambient light would be at
   the wedding venue when we set everything up.
3. Can be "turned on and off" by changing it's color/light emission
   somehow. This just makes drawing/writing a lot easier, since never
   being able to "turn off" the color would be like drawing with a
   pencil you couldn't pick up off the page.
4. Comes in multiple colors, so we can draw a few different colored
   lines/effects on the screen.

These requirements suggested using light-producing objects, like
glowsticks, LEDs, or lightbulbs, rather than something like a ball
painted in a bright color. An light-producing object works more
consistently in different light conditions, since it doesn't need to
reflect light from it's environment to look colored. It's also easy to
turn on and off. An LED or lightbulb can just be powered on and off,
where as something that doesn't light itself up has to be occluded
somehow, which is much tricker.

After a few false starts with glowsticks and smaller LEDs, we found
that colored lightbulbs worked quite well:

<video autoplay loop
src="/assets/videobooth/james_first_heart.mp4"></video>


[^1]: That said, Glitch isn't perfect. We certainly ran into some
difficulties with it later on, as you'll see if you keep reading.
