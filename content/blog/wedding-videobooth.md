Title: Creating a magical videobooth for our friends' wedding
Date: 2018-10-14
Slug: wedding-videobooth
Summary: Some technical and aesthetic notes about building a surprise for the wedding of some friends
Status: draft

*This post was written jointly with
[Rachel Hwang](http://rahwang.strikingly.com/), who worked on this
project with me.*

Our friends Emily and [John](http://www.daz.zone/) got married last
October. A few months beforehand, we decided to make them a surprise
in the form of an interactive computer art installation to be deployed
at their wedding. The guests used it to create some amazing things:

<video width="800" height="500" autoplay loop playsinline muted src="/assets/videobooth/josh_emily_heart_short.mp4"></video>

<video width="800" height="500" autoplay loop playsinline muted src="/assets/videobooth/riva_dance_short.mp4"></video>

<video width="800" height="500" autoplay loop playsinline muted src="/assets/videobooth/nick_star_short.mp4"></video>

This post is about what we did to make it, some of the technical and
aesthetic problems we ran into along the way, and how we solved them.


## Getting started

We started by thinking about the important characteristics of the
final product and brainstorming some ideas. Our goal was to make
something that, in addition to being a fun interactive toy for guests
at the wedding, would also produce digital artifacts that could be compiled
into a "video guestbook" of sorts for Emily and John.

We settled on the idea of guests moving colored lights through the air
like wands to draw interesting visual effects on a screen in front of
them. You'd walk into the booth, type a message to Emily and John to
be included with your video, and then have 30 seconds to record a
video of yourself making something beautiful, funny, or interesting
using the light wands. The drawing would work via a computer vision
algorithm tracking the lights and rendering effects based on their
positions.

## Color tracking

We needed a way to track the positions of colored objects (called
"blobs" in computer vision terminology) in a video stream. We poked
around for a while and found [tracking.js](https://trackingjs.com/), a
library of JavaScript computer vision algorithms. After a bit of trial
and error we managed to get a very basic demo working:

<video autoplay loop playsinline muted
src="/assets/videobooth/video.mp4"></video>

This was pretty much just the tracking.js
[color tracking example](https://trackingjs.com/examples/color_camera.html)
with the parameters tweaked so it tracks orange blobs rather
cyan/magenta/yellow ones, but it was a start.

## Making it beautiful

Now we needed to draw something more appealing than a bounding box
around the blobs. We decided to use [p5.js](https://p5js.org/) for
drawing, since we were both familiar with it. We added p5 to our
codebase and tweaked the color tracking code to use the `canvas` and
`video` elements generated by p5 rather than creating it's own, and to
be driven by p5's render loop. We started off by drawing a basic
flower pattern using ellipses. We don't have video of this stage
unfortunately, but here are some photos:

![James with flower](/assets/videobooth/james_with_yellow_flower.png)
![Rachel with flower](/assets/videobooth/rachel_with_yellow_flower.png)

Again, not the most beautiful thing, but better than a bounding box!
It's also a proof of concept that we could integrate p5 with
tracking.js and start working towards something better.

In searching for inspiration, Golan Levin's
[Yellowtail](http://flong.com/projects/yellowtail/), which is one of
the examples included with Processing, caught our eye. We were able to
find [a Yellowtail implementation using
p5](https://n1ckfg.github.io/yellowtails/p5js/) and the [source code
for it](https://github.com/n1ckfg/yellowtails). We decided to try and
integrate this code into our project, adapting the callbacks for
things like clicks and mouse movement to instead by triggered by the
tracked blobs. This proved to be pretty challenging and lead to a lot
of pitfalls and visually interesting bugs, for example:

![James demoing bug](/assets/videobooth/james_flower_glitch.png)

Eventually, we managed to get it working:

![James Yellowtail image](/assets/videobooth/james_first_yellowtail.png)

This was finally starting to feel a bit like what we had in out mind's
eye when we started this project.

## Hardware choices

One of the big hurdles remaining was figuring out what we were going
to use as the colored objects that the wedding guests would draw
with. Until now, we'd been testing with ceiling lights and various
shiny objects in our apartments, but that wasn't going to cut it for
the wedding. We wanted whatever we settled on to have a few
characteristics:

1. Minimal false positives, i.e. someone's clothes or body won't be
   mistaken for it, resulting in spurious drawing.
2. Works well in a variety of light conditions. This was important
   since we weren't sure what the ambient light would be like
   at the wedding venue.
3. Can be "turned on and off" by changing it's color/light emission
   somehow. This just makes drawing/writing a lot easier, since never
   being able to "turn off" the color would be like drawing with a
   pencil you couldn't pick up off the page.
4. Comes in multiple colors, allowing for multiple distinct colored
   lines or effects.

These requirements suggested using light-producing objects, like
glowsticks, LEDs, or lightbulbs, rather than something like a ball
painted in a bright color. A light-producing object works more
consistently in different ambient light conditions, since it doesn't
need to reflect light from it's environment to look colored. An LED or
lightbulb can just be powered on and off, where as something that
doesn't light itself up has to be occluded somehow in order to stop
drawing, which is much tricker.

After a few false starts with glowsticks and smaller LEDs, we found
that colored lightbulbs worked quite well:

<video autoplay loop playsinline muted
src="/assets/videobooth/james_first_heart.mp4"></video>

<video autoplay loop playsinline muted
src="/assets/videobooth/james_writing_john.mp4"></video>

Getting to this point was really exciting! It felt like we were
getting close to something we'd be happy with displaying at the
wedding. As they say though, the last 10% of the work often takes 90%
of the time.

## From one color to three

The videos above used only a red lightbulb. To detect it, we first
tried configuring tracking.js to just find red blobs, but this didn't
work. Colored bulbs are bright enough that a webcam just sees them as
white, so we had to configure tracking.js to find white blobs instead.

We wanted guests to be able to draw with multiple bulbs of different
colors, so just finding white blobs wouldn't work for the final
product, since it would prevent us from distinguishing e.g. a red bulb
from a blue one. Our next challenge was figuring out how to do this.

We ended up getting red, green, and blue LED bulbs. Our plan for
telling the three colors apart involved the subtle "corona" of light
that surrounds colored bulbs in the webcam. Looking at the videos
above, you can see that the while the body of the bulb looks white,
the background pixels surrounding it look pretty red. We took
advantage of this in our initial algorithm for telling the three
bulbs apart:

1. Configure tracking.js to find all white blobs in each frame of
   video.
2. For each blob, add up the R, G, and B values of all the pixels in
   it, ignoring the white pixels (so we're only considering the
   bulb's surroundings, and not the bulb itself).
3. Label the green bulb as the blob with the largest total green
   value, the red bulb as the one with the largest total red value,
   etc.

This did not work very well:

<video autoplay loop playsinline muted
src="/assets/videobooth/james_blue_green_not_working.mp4"></video>

The problem was that the bulbs were too bright, so they illuminated
each other, which made them harder to tell apart. The brightness of
the bulbs also caused other problems:

<video autoplay loop playsinline muted
src="/assets/videobooth/james_face_confusion.mp4"></video>

James' face is so illuminated that the algorithm thinks it's a bulb.

This seemed like a big challenge at first, and we struggled for a
while to come up with tweaks we could make to the algorithm, or some
sort of clever transformation of the pixel data we could use to tell
the colors apart.

Eventually, we took a step back and realized we had been too focused
on finding a software solution to the problem. Rather than forcing our
our program to deal with the too-bright bulbs, we could just make them
dimmer. The solution we eventually settled for doing this on was dirt
simple: cover the bulbs with socks!

<video autoplay loop playsinline muted
src="/assets/videobooth/james_socks_on_bulbs.mp4"></video>

Much better! The socks make the bulbs dim enough that they don't
illuminate each other and their surroundings enough to confuse our
algorithm.

Unfortunately, this didn't solve all our problems. The color profiles
of the bulbs didn't line up as neatly as we'd hoped with the RGB
values the webcam measured. The simple strategy of identifying,
e.g. the greenest blob as the green bulb was pretty
unreliableâ€”sometimes the blue bulb actually had the most
green. Similar problems happened with all three colors.

What we ended up doing instead was manually measuring the typical
color profiles of in each bulb and hard-coding these ranges into our
code. So the algorithm for identifying, e.g. the green bulb now looked
like:

1. Configure tracking.js to find white blobs.
2. For each blob:
    1. Remove the white pixels and sum up the R, G, and B
       values.
    3. For each color, check if it's total value falls within
       the hard-coded typical range for the green bulb.
    4. If each colors is within it's typical range, identify this blob
       as the green bulb.

We also had to filter based on shape to avoid choosing other objects
that were just being illuminated by the bulbs (faces, curtains, etc.),
rather than the bulbs themselves. We removed from consideration any
blob whose width was too different from it's height, since the bulbs
tended to look pretty square from any angle, whereas illuminated
faces, curtains, etc. tended to have one side longer than the other.

One problem we still had was momentary fluctuations in the bulbs'
color profiles. If something else in the scene, like a shirt or a
face, looked more green than the green bulb for even a single frame,
the algorithm would get confused about the green bulb's location,
resulting in weird, jittery drawing paths, and lines jumping across
the screen.

To solve this problem, we ended up implementing "blob persistence" as
described in [this Dan Shiffman
video](https://www.youtube.com/watch?v=r0lvsMPGEoY&t=0s&index=7&list=PLRqwX-V7Uu6aG2RJHErXKSWFDXU4qo_ro)[^2],
which was surprisingly straightforward. This let us keep track of the
"history" of a blob, so that our bulb identification algorithm could
consider not just what a blob looks like in this frame of video, but
what it looked like in previous frames also.

We tweaked our code to use a running average of the red, green, and
blue totals for each blob over the last 50 frames, rather than only
considering the current frame. This meant that if a bulb's color
fluctuated a bit for a frame or two, it wouldn't throw everything off,
since the running average wouldn't be affected much. This made things
a lot less jittery.  We were now able to track all three colors pretty
reliably:

<video autoplay loop playsinline muted
src="/assets/videobooth/james_all_three.mp4"></video>

## Getting even smarter using statistics

Our algorithm was still very sensitive to ambient light conditions. As
the environment got darker or lighter, the color values that the
webcam measured for each bulb changed substantially. This meant that
whenever the lighting changed, the algorithm would stop working, since
the "typical" color profiles that we'd hard-coded for each bulb were
only "typical" in very specific light conditions. In order to get
things working again, we would have to manually change the code to
adapt to the new light conditions, which was time consuming since our
code was getting pretty gnarly at this point.

There was no way to know what the light conditions at the wedding
venue would be like in advance, so if we didn't come up with a better
system, we'd have to do this tweaking while setting the booth up. We
were worried that we simply wouldn't have time for this, given all the
hectic nature of weddings.

What we needed was a way for our algorithm to adapt to
different light conditions automatically, without us having to spend
time manually adjusting it.

We decided to try adding a way to "calibrate" the algorithm before
starting it running:

1. While setting the botth up, we'd hold up the blue bulb and click on
   it to tell the system "this blob is the blue bulb, use it to
   calibrate the detection algorithm".
2. We'd move the bulb around a bit while the software would record the
   maximum and minimum observed total values of red, green, and blue.
3. Repeat for the other two colors.
4. While the booth was running, any detected blob whose total color
   values fell within the recorded values for a bulb would be
   identified as that bulb.

This seemed like it could work, but it also felt brittle and
ad-hoc. We took a step back and realized that the right way to think
about the problem was in terms of probability. It isn't the case that
a blob goes from a 100% chance of being the green bulb to a 0% chance
when it's value for a particular color strays slightly outside the
sampled range. Rather, a blob has some probability of being the green
bulb that slides smoothly from 0% to 100% based on the overall
similarity of it's color profile to that measured for the green bulb.

We're far from experts on statistics, but figured there had to be a
statistical method that would let us compute probability of a blob
being a particular bulb based on the overall similarity of the blob's
measured color values to the sampled data for the bulb.

We eventually realized that what we were wanted to do, in statistical
terms, was determine the probability that two samples were drawn from
the same underlying distribution. In our case, the underlying
distribution is the red, green, and blue values that the webcam tends
to measure for a given bulb. One of the samples is the red, green, and
blue values measured during calibration of that bulb, and the other
sample the is red, green, and blue values from the frames we observe
for an as-yet unidentified blob being processed by our algorithm. If,
we can determine the probability that the unidentified blob sample is
from the same underlying distribution as each of the calibration
samples for the three bulbs, we know the unidentified blob's
probability of being each bulb!

It turns out there is a statistical test that does exactly this: the
two-sample [Kolmogorovâ€“Smirnov
test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).

At this point, we were excited about the idea because it seemed like
such an elegant, principled solution to the problem, but skeptical,
since in programming it so often turns out that the elegant,
principled solution doesn't work in practice, due to performance, the
particulars of your problem not fitting into an algorithm in the way
you thought they would, etc. We decided to go for it anyway and see
what happened.

We were able to find a preexisting JavaScript implementation of the
Kolmogorovâ€“Smirnov test in the [Jerzy
library](https://github.com/pieterprovoost/jerzy) by Pieter Provoost,
which we hacked into our project. We ended up having to do a few dirty
tricks to get this to work

no multiparameter version,we have multiple parameters (red, gree,
blue, squareness, etc.)

there's a research paper but decided that implementing a novel
algorithm from scratch was a bit out of scope

just added up [D
values](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov%E2%80%93Smirnov_statistic),
which I'm sure is wildly statistically unjustified, but ended up
working pretty well in practice




## Performance optimizations




## The final product




[^1]: That said, Glitch isn't perfect. We certainly ran into some
difficulties with it later on, as you'll see if you keep reading.

[^2]: Many of Dan's [computer vision
tutorials](https://www.youtube.com/playlist?list=PLRqwX-V7Uu6aG2RJHErXKSWFDXU4qo_ro)
were really helpful for understanding what we were doing throughout
this whole project.
