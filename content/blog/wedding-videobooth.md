Title: Creating a magical videobooth for a wedding
Date: 2018-10-14
Slug: wedding-videobooth
Summary: Some technical and aesthetic notes about building a surprise for the wedding of some friends
Status: draft

*This post was written jointly with
[Rachel Hwang](http://rahwang.strikingly.com/), who worked on this
project with me.*

Our friends [Emily](https://www.instagram.com/emilytishay) and
[John](http://www.daz.zone/) got married recently. A few months
beforehand, we decided to make them a surprise in the form of an
interactive computer art installation to be deployed at their
wedding. The guests used it to create some amazing things:

<video width="800" height="500" autoplay loop src="/assets/videobooth/josh_emily_heart_short.mp4"></video>

<video width="800" height="500" autoplay loop src="/assets/videobooth/riva_dance_short.mp4"></video>

<video width="800" height="500" autoplay loop src="/assets/videobooth/nick_star_short.mp4"></video>

This post is about what we did to make it, some of the technical and
aesthetic problems we ran into along the way, and how we solved them.


## Getting started

We first thought about the important characteristics of the final
product, and brainstorming some ideas. Our goal was to make something
that, in addition to being a fun interactive toy for guests at the
wedding, would also produce artifacts that could be compiled into a
"guestbook" of sorts for Emily and John.

We settled on the idea of guests moving colored lights through the air
like wands to draw interesting visual effects on a screen in front of
them, by means of a computer vision algorithm tracking the lights and
rendering effects based on their positions. You'd walk into the booth,
type a message to Emily and John to be included with your video, and
then have 30 seconds to record a video of yourself making something
beautiful, funny, or interesting using the light wands.

## Settling on a development environment

Now that we had a rough idea, we had to actually start coding up some
prototypes. Both of us struggle with getting over the initial
"activation energy" hump in starting projects like this, but find it
much easier to keep going once we've started.

To help with this, we ended up choosing [Glitch](https://glitch.com/)
as our development environment, which was an amazing choice. Glitch
makes the initial phases of working on a web project with someone else
*incredibly easy*. Collaboration? Just type in the shared editor and
everyone else's view updates in real-time. Deployment? Already done,
just go to `my-project-name.glitch.me` and see the code you just typed
running live. We didn't believe the hype about Glitch before this
project, but now we get it[^1].

We also chose Glitch because it's web-based. Because the web is such a
flexible, powerful development platform, and because we're both
familiar with it, we were more confident in our ability to quickly do
some of the ancillary but important parts of the project (user
interface for typing messages, capturing and saving videos) than if we
had chosen something like [Processing](https://processing.org/).


## Color tracking

As a prerequisite for doing any of this, we needed a way to track the
positions of colored objects in a video stream. We poked around for a
while and found [tracking.js](https://trackingjs.com/), a library of
JavaScript computer vision algorithms. After a bit of trial and error
we managed to use it's color tracking facilities to get a very basic
demo working:

<video autoplay loop
src="/assets/videobooth/video.mp4"></video>

This was pretty much just the tracking.js
[color tracking example](https://trackingjs.com/examples/color_camera.html)
with the parameters tweaked so it tracks orange rather
cyan/magenta/yellow, but it was a start.

## Making it beautiful

Now we needed to draw something more appealing than a bounding box in
response to the user's movements. We decided to use
[p5.js](https://p5js.org/) for drawing, since we were both somewhat
familiar with it. We added p5 to our codebase and tweaked the color
tracking code to use `canvas` and `video` elements generated by p5,
and to be driven by p5's render loop. We started off by drawing a
basic flower pattern using ellipses. We don't have video of this stage
unfortunately, but here are some photos:

![James with flower](/assets/videobooth/james_with_yellow_flower.png)
![Rachel with flower](/assets/videobooth/rachel_with_yellow_flower.png)

Again, not the most beautiful thing, but better than a bounding box!
It's also a proof of concept that we could integrate p5 with
tracking.js and start working towards something better.

In searching for inspiration, Golan Levin's
[Yellowtail](http://flong.com/projects/yellowtail/), which is one of
the examples included with Processing, caught our eye. We thought the
strokes it generates might look nice if we could adapt them to follow
the path of the tracked blobs on the camera, rather than the path of
the mouse.

We were able to find
[a Yellowtail implementation using p5](https://n1ckfg.github.io/yellowtails/p5js/)
and the
[source code for it](https://github.com/n1ckfg/yellowtails). We
decided to try and integrate this code into our project, adapting the
callbacks for things like clicks and mouse movement to instead by
triggered by the blobs detected by the color tracker. This proved to
be pretty challenging and have a lot of pitfalls and visually
interesting bugs, for example:

![James demoing bug](/assets/videobooth/james_flower_glitch.png)

Eventually, we managed to get it working:

![James Yellowtail image](/assets/videobooth/james_first_yellowtail.png)

This was finally starting to feel a bit like what we had in out mind's
eye when we started this project.

## Hardware choices

One of the big hurdles remaining was figuring out what we were going
to use as the colored objects to be tracked in the final setup. Up
until this point, we'd been testing with ceiling lights and various
shiny objects in out apartments, but that wasn't going to cut it for
the final version. We wanted whatever we settled on to have a few
characteristics:

1. Minimal false positives, i.e. someone's clothes or body won't be
   mistaken for it, resulting in spurious drawings.
2. Works well in a variety of light conditions. This was important
   since we weren't exactly sure what the ambient light would be like
   at the wedding venue when we set everything up.
3. Can be "turned on and off" by changing it's color/light emission
   somehow. This just makes drawing/writing a lot easier, since never
   being able to "turn off" the color would be like drawing with a
   pencil you couldn't pick up off the page.
4. Comes in multiple colors, so we can draw a few distinct colored
   lines/effects on the screen.

These requirements suggested using light-producing objects, like
glowsticks, LEDs, or lightbulbs, rather than something like a ball
painted in a bright color. An light-producing object works more
consistently in different light conditions, since it doesn't need to
reflect light from it's environment to look colored. It's also easy to
turn on and off. An LED or lightbulb can just be powered on and off,
where as something that doesn't light itself up has to be occluded
somehow, which is much tricker.

After a few false starts with glowsticks and smaller LEDs, we found
that colored lightbulbs worked quite well:

<video autoplay loop
src="/assets/videobooth/james_first_heart.mp4"></video>

<video autoplay loop
src="/assets/videobooth/james_writing_john.mp4"></video>

Getting to this point was really exciting! It felt like we were
getting close to something we'd be happy with displaying at the
wedding. As they say though, the last 10% of the work takes 90% of the
time.

## From one color to three

The videos above were done with one red lightbulb from a local
hardware store. In spite of this, the color tracking algorithm is
actually looking for a *white* blob, since lightbulbs are so bright
that they look white in the webcam, regardless of what color they're
painted.

This meant our next challenge was to be able to distinguish lightbulbs
of different colors from each other, so guests would be able to draw
in multiple colors.

We ended up getting red, green, and blue LED bulbs, on the theory that
they would be the easiest for the computer vision algorithm to
distinguish from each other. Our plan for telling the colors apart
involved the subtle "corona" of light that surrounds colored bulbs in
the webcam. Looking at the videos above, you can see that the while
the bulb itself looks white, the background pixels immediately
surrounding the bulb look pretty red. So our algorithm for
distinguishing them was something like:

1. Configure tracking.js to find all white blobs in the frame.
2. For each blob, add up the R, G, and B values of all the pixels in
   it, ignoring all of the white pixels, so we just consider the
   bulb's immediate surroundings, and not the bulb itself.
3. Label the blue bulb as the blob in which blue is the most
   over-represented relative to green and red, the green bulb as the
   one in which green is the most overrepresented, etc.

This did not work out as planned:

<video autoplay loop
src="/assets/videobooth/james_blue_green_not_working.mp4"></video>

The problem was that the bulbs were too bright, which made them harder
to tell apart. Distinguishing red from blue or green worked OK, but as
you can see the algorithm had a very hard time telling the blue and
green bulbs apart, and kept switching it's assignment of blue between
the two bulbs. The fact that the bulbs were too bright also caused
other problems:

<video autoplay loop
src="/assets/videobooth/james_face_confusion.mp4"></video>

Here you can see the bulb illuminating James' face so much that the
algorithm thinks his face is a bulb.

This seemed like a big challenge at first, and we struggled for a
while to come up with tweaks we could make to the algorithm, or some
sort of clever transformation of the pixel data we could use to tell
the colors apart.

The solution we eventually settled on was so dirt simple it seemed
obvious in retrospect: put socks on the lightbulbs to make them
dimmer!

<video autoplay loop
src="/assets/videobooth/james_socks_on_bulbs.mp4"></video>

Much better. We still needed to do a bit of adjustment to the
algorithm. The color profiles of the green and blue bulbs were similar
enough that choosing the "most green" blob as the green bulb and "most
blue" blob as the blue one wasn't super successful. We ended up having
to choose some arbitrary, experimentally determined "threshold" values
of blue and green that each bulbs tended to fall within, and identify
them based on these. We also had to add a bit more filtering to avoid
choosing other objects that were just being illuminated by the bulbs,
rather than the bulbs themselves. This took the form of filtering out
any blobs that were "too rectangular" based on the ratio of their
widths to heights, since the bulbs tend to appear pretty square no
matter how you hold them, and illuminated faces, curtains, etc. tended
to have one side longer than the other.

One problem we still had was that the over-representations of the
three colors fluctuated pretty substantially from frame to frame. If a
single frame had the blue bulb looking a bit less blue than another
blob, the algorithm would reassign it's location, resulting in weird,
jittery drawing paths. The problem was that the algorithm had not
conception of history. If it could look at the frames immediately
before the current one and average them together, it would be pretty
obvious which bulb was the bulb one.

We ended up implementing the "blob persistence" algorithm described in
[this Dan Shiffman
video](https://www.youtube.com/watch?v=r0lvsMPGEoY&t=0s&index=7&list=PLRqwX-V7Uu6aG2RJHErXKSWFDXU4qo_ro)[^2],
which was surprisingly straightforward. We then tweaked the algorithm
to use the average R,G, and B totals from the last 50 frames, rather
than just the current frame, which made things a lot less jittery. We
were now able to track all three blobs pretty reliably!

<video autoplay loop
src="/assets/videobooth/james_all_three.mp4"></video>

## Getting even smarter using statistics

One major problem we still had was that our algorithm was very
sensitive to ambient light conditions. As the environment got darker
or lighter, the R, G, and B values that the webcam measured for the
bulbs change fairly dramatically, which meant we had to do a lot of
fiddling with the detection thresholds hard-coded in the algorithm to
make things work reliably whenever the lighting changed. Our code was
getting fairly gnarly at this point, so making these adjustments was
time-consuming and error-prone.

We were worried that we simply wouldn't have time to make these
adjustments on-site at the wedding venue during the final setup. We
were both in the wedding party, so we knew we'd have a lot of other
responsibilities and therefore not a lot of time. We also knew we'd
have a lot of other work to do just to set the booth up, between
plugging in power strips and extension cords, connecting the computer
to the monitor and projector we were using as displays, putting up
signs so guests would understand what to do, etc.

Our code was definitely going to have to adapt to the light
conditions at the venue when we got there, since we had no way of
knowing what they'd be beforehand. What we needed was a way to make
this process much faster than tweaking some parameters in our code by
trial and error.

What figured we could probably automate the process of figuring out
the R, G, and B values that each bulb tended to fall within by adding
a "calibration" mode. The way this would work:

1. We'd hold up the blue bulb and tell the software "this blob is the
   blue bulb, use it to calibrate the detection algorithm".
2. We'd move the bulb around a bit while the software records the RGB
   values of the pixels in that blob.
3. The software would measure typical RGB values from those frames,
   and feed them to the bulb detection algorithm to use for
   identifying the blue bulb.
4. Repeat for the other two colors.

This seemed like it could work, but it also seemed fairly ad-hoc. This
process felt really reminiscent of some things for the world of
statistics, like sampling, distinguishing populations from each,
etc. We were convinced there had to be a more principled way to do
this using a statistical method of some sort, and it turns out there
is!

We eventually realized that what we were wanted to do, in statistical
terms, was determine whether or not two samples were drawn from the
same underlying distribution. In our case, the underlying distribution
is the RGB values that the webcam tends to see for a given bulb. One
of the samples is the set the RGB values from the frames taken during
the calibration phase, and the other sample the is RGB values from the
frames we observe of an as-yet unidentified blob being processed by
our algorithm. We know which population (bulb) the "calibration"
sample was taken from, so if we can determine how likely it is that
the pixels we observe for any given blob were drawn from the same underlying
population, we've determined how likely it is to be that bulb!

It turns out there is a statistical test that does exactly this: the
two-sample [Kolmogorov–Smirnov
test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).

At this point, we were excited about the idea because it seemed like
such an elegant, principled solution to the problem, but skeptical,
since in programming it so often turns out that the elegant,
principled solution doesn't work in practice, due to performance, the
particulars of your problem not fitting into an algorithm in the way
you thought they would, etc. We decided to go for it anyway and see
what happened.

We were able to find a preexisting JavaScript implementation of the
Kolmogorov–Smirnov test in the [Jerzy
library](https://github.com/pieterprovoost/jerzy) by Pieter Provoost,
which we hacked into our project. We ended up having to do a few dirty
tricks to get this to work

no multiparameter version,we have multiple parameters (red, gree,
blue, squareness, etc.)

there's a research paper but decided that implementing a novel
algorithm from scratch was a bit out of scope

just added up [D
values](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov%E2%80%93Smirnov_statistic),
which I'm sure is wildly statistically unjustified, but ended up
working pretty well in practice




## Performance optimizations




## The final product




[^1]: That said, Glitch isn't perfect. We certainly ran into some
difficulties with it later on, as you'll see if you keep reading.

[^2]: Many of Dan's [computer vision
tutorials](https://www.youtube.com/playlist?list=PLRqwX-V7Uu6aG2RJHErXKSWFDXU4qo_ro)
were really helpful for understanding what we were doing throughout
this whole project.
